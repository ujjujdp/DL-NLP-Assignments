{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "P-VOBcURXBvh"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanifold\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TSNE\n\u001b[1;32m      7\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Importing necessary packages\n",
    "import spacy\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "am_mu_m5nFDA",
    "outputId": "0c01e781-c94c-4991-e6af-659860c14ccb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ujjawal/anaconda3/envs/ujju/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uc85pf1GCliP"
   },
   "source": [
    "# Functions for sentence preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9NhmOdJKCRKc"
   },
   "outputs": [],
   "source": [
    "def remPunc(text):\n",
    "  if type(text) == str:\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    text = re.sub('[\\W]+', '', text.lower())\n",
    "    return text\n",
    "\n",
    "def refineSentence(tempSentence):\n",
    "  tokens = nlp(str(tempSentence))\n",
    "  filtered = []\n",
    "  for token in tokens:\n",
    "    if (token.is_stop == False):\n",
    "      textAfterRemPunc = remPunc(token.lemma_)\n",
    "      if(textAfterRemPunc != ''):\n",
    "        filtered.append(textAfterRemPunc)\n",
    "  return filtered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "a-0T9Bj1a5D1"
   },
   "outputs": [],
   "source": [
    "def posTo1Negto0(tempStr):\n",
    "  if(tempStr == \"positive\"):\n",
    "    return 1\n",
    "  return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2rYV2wbIQAa"
   },
   "source": [
    "# Getting GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mrehafRAIQu8"
   },
   "outputs": [],
   "source": [
    "embeddingsDict = {}\n",
    "dim = 300\n",
    "with open(\"./Data/glove.6B.300d.txt\", 'r', encoding=\"utf-8\") as wordEmbeddings:\n",
    "  for line in wordEmbeddings:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:],'float32')\n",
    "    embeddingsDict[word]=vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnl8Zb6XICg9"
   },
   "source": [
    "Function to convert refined sentence to sentence vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "AD9qJkKhID3j"
   },
   "outputs": [],
   "source": [
    "def sentenceToWordEmbeddings(preProccessedList):\n",
    "  preProccessedListVectors = []\n",
    "  for preProcessedWordList in preProccessedList:\n",
    "    tempList = []\n",
    "    for word in preProcessedWordList:\n",
    "      if(word in embeddingsDict):\n",
    "        tempList.append(embeddingsDict[word])\n",
    "    preProccessedListVectors.append(np.array(tempList).T)\n",
    "  return preProccessedListVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_iPyuUYsIv2S"
   },
   "source": [
    "Function for Raw Data to TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qbv2yssrHO0r"
   },
   "outputs": [],
   "source": [
    "def findTensorDataset(tempData,n_samples):\n",
    "  tempData = np.array(tempData)[:n_samples]\n",
    "\n",
    "  # Splitting Data into X and y\n",
    "  X_sentences = tempData[:,0]\n",
    "  y = [posTo1Negto0(x) for x in tempData[:,1]]\n",
    "  \n",
    "  # PreProcessing the Train Data\n",
    "  preProccessedList = [refineSentence(sentence) for sentence in X_sentences]\n",
    "\n",
    "  # Training Data to wordsVector Matrix -> WordsVector is a list of 2d tensors of data samples, since we can't create vairable length 3d tensors.\n",
    "  wordsVector = sentenceToWordEmbeddings(preProccessedList)\n",
    "\n",
    "  # Adding padding to the data-set\n",
    "  max_len = max([i.shape[1] for i in wordsVector])\n",
    "\n",
    "  X = []\n",
    "  for i in wordsVector:\n",
    "    curr_len = i.shape[1]\n",
    "    temp = np.zeros((dim,max_len-curr_len))\n",
    "    X.append(np.concatenate((i, temp),axis = 1).T)\n",
    "  X = np.array(X)\n",
    "\n",
    "  # Creating DataLoaders\n",
    "  X = torch.Tensor(X)\n",
    "  y = torch.Tensor(y)\n",
    "  y = y.type(torch.LongTensor)\n",
    "\n",
    "  y = y.type(torch.LongTensor)\n",
    "  tempDatasetTensor = TensorDataset(X,y) # create your datset\n",
    "  return tempDatasetTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIo_EnH-Lr3u"
   },
   "source": [
    "HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KL8s3NQ4mwY7"
   },
   "outputs": [],
   "source": [
    "# HyperParameters\n",
    "batchSize = 32\n",
    "epochs = 10\n",
    "hidden_size = 512\n",
    "input_size = dim\n",
    "# seq_size = dim\n",
    "num_classes = 1\n",
    "num_layers = 1\n",
    "lr = 1e-2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFgn961VJqsM"
   },
   "source": [
    "TRAIN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQSVBI0kXPPb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n"
     ]
    }
   ],
   "source": [
    "trainPath = \"./Data/Ass2/train.csv\"\n",
    "trainData = pd.read_csv(trainPath)\n",
    "trainData = trainData.values.tolist()\n",
    "\n",
    "n_samples = len(trainData)#//10\n",
    "print(n_samples)\n",
    "trainDatasetTensor = findTensorDataset(trainData, n_samples)\n",
    "train_size = int(n_samples*(0.9))\n",
    "val_size = n_samples - train_size\n",
    "trainData, validData = random_split(trainDatasetTensor, [train_size,val_size])\n",
    "\n",
    "trainDataLoader = DataLoader(trainData, batch_size = batchSize)\n",
    "validDataLoader = DataLoader(validData, batch_size = batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tR_nfpbmt2P"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "    super(LSTM, self).__init__()\n",
    "    self.num_layers = num_layers\n",
    "    self.hidden_size = hidden_size\n",
    "    \n",
    "    self.LSTM = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, bias = True, batch_first = True, dropout = 0, bidirectional = False )\n",
    "\n",
    "    self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x):\n",
    "    h0 = (  torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(device), torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(device) )\n",
    "    out, _ = self.LSTM(x, h0)\n",
    "    out = out[:,-1]\n",
    "    out = self.fc(out)\n",
    "    out = self.sigmoid(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qy4tjuvrqYga"
   },
   "outputs": [],
   "source": [
    "class SA_LSTM:\n",
    "  def __init__(self, input_size, hidden_size, num_layers, num_classes, epochs,lr):\n",
    "    self.input_size = input_size\n",
    "    self.num_layers = num_layers\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_classes = num_classes\n",
    "    self.epochs = epochs\n",
    "    \n",
    "    self.NNobj = LSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "    self.cross_entropy_loss = nn.BCEWithLogitsLoss()\n",
    "    self.optimizer = optim.SGD(self.NNobj.parameters(), lr = lr)\n",
    "    \n",
    "    self.trainLossList = []\n",
    "    self.validLossList = []\n",
    "    self.trainLossListPlotting = []\n",
    "    self.validLossListPlotting = []\n",
    "\n",
    "  def fit(self, trainDataLoader, validDataLoader):\n",
    "    for i in range(self.epochs):\n",
    "      self.trainLossList = []\n",
    "      self.validLossList = []\n",
    "      accuracyListTrain = []\n",
    "      countBatch = 0\n",
    "      totalBatch = n_samples // batchSize\n",
    "      for batch in trainDataLoader:\n",
    "        print('[',countBatch,'/',totalBatch,']')\n",
    "        countBatch += 1\n",
    "        self.NNobj.train()\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # b = x.size(0)\n",
    "        # x = x.view(b, -1)\n",
    "\n",
    "        forwardValue = self.NNobj(x)\n",
    "        costFunction_J = self.cross_entropy_loss(forwardValue.reshape(forwardValue.shape[0],1), y.float().reshape(y.shape[0],1))\n",
    "        self.NNobj.zero_grad()\n",
    "        costFunction_J.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(self.NNobj.parameters(), 3)\n",
    "\n",
    "        self.optimizer.step()\n",
    "        self.trainLossList.append(costFunction_J.item())\n",
    "        accuracyListTrain.append(y.eq(forwardValue.detach().argmax(dim = 1)).float().mean())\n",
    "          \n",
    "      accuracyListVal = []\n",
    "      for batch in validDataLoader:\n",
    "        self.NNobj.eval()\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "          forwardValue = self.NNobj(x)\n",
    "          costFunction_J = self.cross_entropy_loss(forwardValue.reshape(forwardValue.shape[0],1), y.float().reshape(y.shape[0],1))\n",
    "        self.validLossList.append(costFunction_J.item())\n",
    "        accuracyListVal.append(y.eq(forwardValue.detach().argmax(dim = 1)).float().mean())\n",
    "      \n",
    "      self.trainLossListPlotting.append(torch.tensor(self.trainLossList).mean())\n",
    "      self.validLossListPlotting.append(torch.tensor(self.validLossList).mean())\n",
    "      accuracyTrain = torch.tensor(accuracyListTrain).mean() * 100\n",
    "      accuracyVal = torch.tensor(accuracyListVal).mean() * 100\n",
    "\n",
    "      print('At Epoch Number: ' + str(i+1) +'; Train Loss= ' + str(\"{:.2f}\".format(torch.tensor(self.trainLossList).mean()))+'; Validation Loss= ' + str(\"{:.2f}\".format(torch.tensor(self.validLossList).mean())) + \"; \" + \"Train Accuracy = \", \"{:.2f}\".format(accuracyTrain), \"%\" + \"; \" + \"Validation Accuracy = \", \"{:.2f}\".format(accuracyVal), \"%\")\n",
    "\n",
    "  def predict(self, testDataLoader):\n",
    "    accuracyListPred = []\n",
    "    for batch in testDataLoader:\n",
    "      x, y = batch\n",
    "      x, y = batch\n",
    "      x, y = x.to(device), y.to(device)\n",
    "      with torch.no_grad():\n",
    "        forwardValue = self.NNobj(x)\n",
    "          \n",
    "      accuracyListPred.append(y.eq(forwardValue.detach().argmax(dim = 1)).float().mean())\n",
    "    accuracy = torch.tensor(accuracyListPred).mean() * 100\n",
    "    print('\\033[1m' + \"Review Classification Accuracy on Test Data = \", \"{:.2f}\".format(accuracy), \" %\" + '\\033[0m' )\n",
    "\n",
    "  def plotLossCurve(self, flag):\n",
    "    x = [(i+1) for i in range(self.epochs)]\n",
    "    plt.xlabel('#Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    tempStr = \"Loss curve for \"\n",
    "    if(flag == 0):\n",
    "        tempStr += \"Train Data\"\n",
    "        plt.plot(x,self.trainLossListPlotting)\n",
    "    else:\n",
    "        tempStr += \"Validation Data\"\n",
    "        plt.plot(x,self.validLossListPlotting)\n",
    "    plt.title(tempStr)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "CggZ2laXr1XP",
    "outputId": "5650414a-4819-4603-c93e-bdc8c84dd663"
   },
   "outputs": [],
   "source": [
    "# Clear CUDA Memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Fitting the Model\n",
    "obj = SA_LSTM(input_size, hidden_size, num_layers, num_classes, epochs,lr)\n",
    "obj.fit(trainDataLoader, validDataLoader)\n",
    "obj.plotLossCurve(0)\n",
    "obj.plotLossCurve(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9AW1TZ2GuXa"
   },
   "source": [
    "Loading Test Data & Finding Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJ8MU_P4Gyuu"
   },
   "outputs": [],
   "source": [
    "testPath = \"./Data/Ass2/test.csv\"\n",
    "testData = pd.read_csv(trainPath)\n",
    "testData = testData.values.tolist()\n",
    "\n",
    "testDatasetTensor = findTensorDataset(testData)\n",
    "testDataLoader = DataLoader(testDatasetTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-gBuxOM8sSRc"
   },
   "outputs": [],
   "source": [
    "obj.predict(testDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
